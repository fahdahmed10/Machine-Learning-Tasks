# ğŸ® Breast Cancer Classification Quest

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)  
[![Python 3.11](https://img.shields.io/badge/Python-3.11-green.svg)](https://www.python.org/)  
[![Build Status](https://img.shields.io/badge/Build-Passing-brightgreen.svg)](https://github.com/yourusername/breast-cancer-classification/actions)  
[![Contributors](https://img.shields.io/badge/Contributors-Welcome-orange.svg)](https://github.com/yourusername/breast-cancer-classification/graphs/contributors)  
[![Last Updated](https://img.shields.io/badge/Last%20Updated-Jul%2030,%202025-lightgrey.svg)](https://github.com/yourusername/breast-cancer-classification)

Embark on a data-driven adventure with `eda.ipynb`! This Jupyter notebook transforms the Wisconsin Breast Cancer dataset into a battleground where machine learning models duel to classify tumors as malignant (M) or benign (B). ğŸ›¡ï¸ğŸ“Š

## ğŸš€ Table of Contents

- [ğŸ¯ Problem Definition](#-problem-definition)
- [ğŸ› ï¸ Solution Approach](#-solution-approach)
- [ğŸ“¦ Installation](#-installation)
- [ğŸ® Usage](#-usage)
- [ğŸ† Results](#-results)
  - [ğŸ“ˆ Model Performance](#-model-performance)
  - [ğŸ–¼ï¸ Confusion Matrices](#-confusion-matrices)
- [ğŸ’¾ Saved Artifacts](#-saved-artifacts)
- [ğŸ¤ Contributing](#-contributing)
- [ğŸŒŸ Future Work](#-future-work)
- [ğŸ“œ License](#-license)

## ğŸ¯ Problem Definition

Breast cancer is a global health boss fight! Early detection is your ultimate power-up. The dataset, packed with features like radius, texture, perimeter, and concavity from biopsy images, challenges us to craft a model that distinguishes malignant from benign tumorsâ€”leveling up medical diagnosis! ğŸ®ğŸ’‰

## ğŸ› ï¸ Solution Approach

Gear up for this quest with a strategic pipeline:

1. **Data Prep âš™ï¸**: Load the dataset with pandas and encode `diagnosis` (M = 1, B = 0).
2. **EDA Exploration ğŸ—ºï¸**: Uncover data secrets with stats and shapes.
3. **Model Showdown ğŸ—¡ï¸**: Train Logistic Regression, SGD Classifier, SVC, KNeighbors Classifier, Decision Tree, XGBClassifier, and RandomForestClassifier. Save the RandomForest champ!
4. **Scoreboard ğŸ“‹**: Compare train/test accuracies, saved to CSV.
5. **Visual Loot ğŸ•¹ï¸**: Unlock confusion matrices in the `plots` folder.

## ğŸ“¦ Installation

â†’ Clone the repo and gear up:
   ```bash
   git clone https://github.com/yourusername/breast-cancer-classification.git
   cd breast-cancer-classification
   ```

â†’ Install your arsenal with `requirements.txt`:
   ```bash
   pip install -r requirements.txt
   ```

â†’ Place `breast_cancer.csv` in the `data` folder.

## ğŸ® Usage

â†’ Launch the quest:
   ```bash
   jupyter notebook eda.ipynb
   ```

â†’ Follow the cells to battle data, train models, and claim your visuals! ğŸ¯

## ğŸ† Results

### ğŸ“ˆ Model Performance

Check the leaderboard! Accuracies show train/test prowess, with notes on overfitting or balance.

| Model                  | Train Accuracy | Test Accuracy | Notes                              |
|------------------------|-----------------|---------------|------------------------------------|
| SVC                    | 97.96%          | 98.84%        | ğŸ¥‡ Top-tier generalization         |
| KNeighborsClassifier   | 100.00%         | 98.84%        | âš ï¸ Overfit risk, but strong!       |
| SGDClassifier          | 96.21%          | 97.67%        | ğŸ¯ Solid consistency               |
| DecisionTreeClassifier | 98.25%          | 97.67%        | âš”ï¸ Well-rounded fighter            |
| XGBClassifier          | 99.13%          | 97.67%        | ğŸ”¥ High train, slight test dip     |
| LogisticRegression     | 98.54%          | 96.51%        | ğŸ›¡ï¸ Steady performer                |
| RandomForestClassifier | 98.54%          | 96.51%        | ğŸ’¾ Saved champ for next level      |

â†’ **Insight**: SVC and KNeighborsClassifier reign with 98.84% test accuracy. KNeighbors might overfit (100% train), while RandomForest (96.51% both sets) is your saved hero for deployment.

### ğŸ–¼ï¸ Confusion Matrices

Unlock these epic visuals in `plots`! They reveal true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN)â€”your battle stats!

- [Comparison](plots/Comparison.png) â†’ See the big fight overview!
- [DecisionTreeClassifier](plots/Confusion_metric_for_DecisionTreeClassifier.png) â†’ 97.67% with balanced TP/TN.
- [KNN](plots/Confusion_metric_for_KNN.png) â†’ 98.84% with rare missteps.
- [LogisticRegression](plots/Confusion_metric_for_LogisticRegression.png) â†’ 96.51% with minor FP/FN.
- [RandomForestClassifier](plots/Confusion_metric_for_RandomForestClassifier.png) â†’ 96.51% with saved glory.
- [SGDClassifier](plots/Confusion_metric_for_SGDClassifier.png) â†’ 97.67% with moderate FP.
- [SVC](plots/Confusion_metric_for_SVC.png) â†’ 98.84% with epic TP/TN.
- [XGBBoost](plots/Confusion_metric_for_XGBBoost.png) â†’ 97.67% with FN focus.

â†’ These are your treasure mapsâ€”critical for spotting weaknesses, especially false negatives in this health quest!

## ğŸ’¾ Saved Artifacts

- **Model**: `models/RandomForestClassifier.pkl` â†’ Your saved hero!
- **Accuracy Table**: `models/accuracy_table.csv` â†’ The full scoreboard.

## ğŸ¤ Contributing

â†’ Join the guild! Fork, branch, and PR your improvements. Report bugs or ideas via issues. ğŸŒ

## ğŸŒŸ Future Work

â†’ Level up the game:
- **Hyperparameter Tuning ğŸ›ï¸**: Boost accuracy with GridSearchCV.
- **Feature Crafting ğŸ› ï¸**: Add derived features or PCA.
- **Cross-Validation ğŸ”„**: Strengthen with k-fold checks.
- **Web Deployment ğŸŒ**: Build a Flask app for real-time battles.
- **Deep Learning ğŸš€**: Try CNNs on image data.

## ğŸ“œ License

This quest is under the [MIT License](LICENSE). Play on! ğŸ®